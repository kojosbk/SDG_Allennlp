{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple baseline with AllenNlP\n",
    "\n",
    "I haven't seen anyone try to use AllenNLP for a kaggle competition before, so I wrote this kernel to show how it could be done.  \n",
    "AllenNLP abstracts away most of the boilerplate code like training loops, loading pretrained embeddings, and keeping track of experiments which lets you write a lot less code. It also lets you change model architectures and hyperparameters by  creating new experiments entirely from configuration files instead of changing the code for each new experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install AllenNLP from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: packages\r\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.6/site-packages (from -r packages/requirements.txt (line 1)) (2.6.0)\r\n",
      "Collecting awscli (from -r packages/requirements.txt (line 2))\r\n",
      "Requirement already satisfied: greenlet in /opt/conda/lib/python3.6/site-packages (from -r packages/requirements.txt (line 3)) (0.4.13)\r\n",
      "Collecting allennlp (from -r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil->-r packages/requirements.txt (line 1)) (1.12.0)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from awscli->-r packages/requirements.txt (line 2)) (0.2.0)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli->-r packages/requirements.txt (line 2)) (0.14)\r\n",
      "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /opt/conda/lib/python3.6/site-packages (from awscli->-r packages/requirements.txt (line 2)) (3.12)\r\n",
      "Collecting botocore==1.12.128 (from awscli->-r packages/requirements.txt (line 2))\r\n",
      "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli->-r packages/requirements.txt (line 2)) (0.3.9)\r\n",
      "Collecting rsa<=3.5.0,>=3.1.2 (from awscli->-r packages/requirements.txt (line 2))\r\n",
      "Collecting sqlparse>=0.2.4 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (4.31.1)\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (3.5.1)\r\n",
      "Collecting pytorch-pretrained-bert>=0.6.0 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (0.8.0)\r\n",
      "Collecting flaky (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.9.126)\r\n",
      "Requirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.0.23)\r\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (3.0.3)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (2.9.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.16.2)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.0.1.post2)\r\n",
      "Collecting moto>=1.3.4 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting overrides (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting responses>=0.7 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting parsimonious>=0.8.0 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (2.21.0)\r\n",
      "Collecting gevent>=1.3.6 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (4.4.3)\r\n",
      "Collecting conllu==0.11 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: flask>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.0.2)\r\n",
      "Requirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.6)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (0.20.3)\r\n",
      "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (1.1.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (2018.4)\r\n",
      "Collecting editdistance (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: spacy<2.2,>=2.0 in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (2.1.3)\r\n",
      "Collecting word2number>=1.1 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting flask-cors>=3.0.7 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting msgpack<0.6.0,>=0.5.6 (from allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from allennlp->-r packages/requirements.txt (line 4)) (3.2.4)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.128->awscli->-r packages/requirements.txt (line 2)) (1.22)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.128->awscli->-r packages/requirements.txt (line 2)) (0.9.4)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli->-r packages/requirements.txt (line 2)) (0.4.5)\r\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp->-r packages/requirements.txt (line 4)) (1.5.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp->-r packages/requirements.txt (line 4)) (39.1.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp->-r packages/requirements.txt (line 4)) (18.1.0)\r\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp->-r packages/requirements.txt (line 4)) (4.1.0)\r\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp->-r packages/requirements.txt (line 4)) (0.6.0)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert>=0.6.0->allennlp->-r packages/requirements.txt (line 4)) (2019.3.12)\r\n",
      "Requirement already satisfied: sphinx>=1.2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (1.7.4)\r\n",
      "Requirement already satisfied: Jinja2>=2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (2.10)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp->-r packages/requirements.txt (line 4)) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp->-r packages/requirements.txt (line 4)) (2.2.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp->-r packages/requirements.txt (line 4)) (1.0.1)\r\n",
      "Collecting pyaml (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting python-jose<3.0.0 (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting cryptography>=2.3.0 (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: boto>=2.36.0 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (2.48.0)\r\n",
      "Requirement already satisfied: werkzeug in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (0.14.1)\r\n",
      "Collecting aws-xray-sdk<0.96,>=0.93 (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting docker>=2.5.1 (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting jsondiff==1.1.1 (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting xmltodict (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (2.0.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp->-r packages/requirements.txt (line 4)) (2019.3.9)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp->-r packages/requirements.txt (line 4)) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp->-r packages/requirements.txt (line 4)) (2.6)\r\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp->-r packages/requirements.txt (line 4)) (1.0.1)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp->-r packages/requirements.txt (line 4)) (0.1.7)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp->-r packages/requirements.txt (line 4)) (7.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp->-r packages/requirements.txt (line 4)) (0.24)\r\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX>=1.2->allennlp->-r packages/requirements.txt (line 4)) (3.7.1)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (0.2.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (2.0.2)\r\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (7.0.4)\r\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (2.6.0)\r\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (0.9.6)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (0.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (1.0.0)\r\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (2.0.1)\r\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp->-r packages/requirements.txt (line 4)) (0.2.4)\r\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (2.2.0)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (1.2.1)\r\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (2.5.3)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (0.7.10)\r\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (1.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (17.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (1.0.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp->-r packages/requirements.txt (line 4)) (1.0)\r\n",
      "Collecting ecdsa<1.0 (from python-jose<3.0.0->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Collecting pycryptodome<4.0.0,>=3.3.1 (from python-jose<3.0.0->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: future<1.0 in /opt/conda/lib/python3.6/site-packages (from python-jose<3.0.0->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (0.17.1)\r\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (1.11.5)\r\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (0.24.0)\r\n",
      "Requirement already satisfied: jsonpickle in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (0.9.6)\r\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk<0.96,>=0.93->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (1.10.11)\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=2.5.1->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (0.56.0)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4))\r\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (5.1.3)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy->allennlp->-r packages/requirements.txt (line 4)) (0.5.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp->-r packages/requirements.txt (line 4)) (2.18)\r\n",
      "Building wheels for collected packages: overrides, parsimonious, jsonnet, word2number, jsondiff\r\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/21/23/1d/40f7a91bd967efaa7e5c73fed44c46892519d095c7164e9751\r\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/63/12/37/b35b1740afb56f0726f444bf57706e63b6f9ad26412bdd9500\r\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/da/b9/f3/0a30c320e338ee29745de4caed174fa709bb48ed49dc6b0db7\r\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/de/88/71/fc18f2bf760d73e76529e4f1d8a859b3e65956cc39f332dd04\r\n",
      "  Building wheel for jsondiff (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/ef/8e/e7/5680b642f906a3c4584201cf9f3dde60165c9c93313038f038\r\n",
      "Successfully built overrides parsimonious jsonnet word2number jsondiff\r\n",
      "\u001b[31mgevent 1.4.0 has requirement greenlet>=0.4.14; platform_python_implementation == \"CPython\", but you'll have greenlet 0.4.13 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: botocore, rsa, awscli, sqlparse, pytorch-pretrained-bert, flaky, pyaml, ecdsa, pycryptodome, python-jose, cryptography, aws-xray-sdk, responses, docker-pycreds, docker, jsondiff, xmltodict, moto, overrides, parsimonious, gevent, conllu, jsonnet, editdistance, word2number, flask-cors, msgpack, allennlp\r\n",
      "  Found existing installation: botocore 1.12.126\r\n",
      "    Uninstalling botocore-1.12.126:\r\n",
      "      Successfully uninstalled botocore-1.12.126\r\n",
      "  Found existing installation: rsa 4.0\r\n",
      "    Uninstalling rsa-4.0:\r\n",
      "      Successfully uninstalled rsa-4.0\r\n",
      "  Found existing installation: cryptography 2.2.2\r\n",
      "    Uninstalling cryptography-2.2.2:\r\n",
      "      Successfully uninstalled cryptography-2.2.2\r\n",
      "  Found existing installation: gevent 1.3.0\r\n",
      "    Uninstalling gevent-1.3.0:\r\n",
      "      Successfully uninstalled gevent-1.3.0\r\n",
      "  Found existing installation: Flask-Cors 3.0.4\r\n",
      "    Uninstalling Flask-Cors-3.0.4:\r\n",
      "      Successfully uninstalled Flask-Cors-3.0.4\r\n",
      "  Found existing installation: msgpack 0.6.1\r\n",
      "    Uninstalling msgpack-0.6.1:\r\n",
      "      Successfully uninstalled msgpack-0.6.1\r\n",
      "Successfully installed allennlp-0.8.3 aws-xray-sdk-0.95 awscli-1.16.138 botocore-1.12.128 conllu-0.11 cryptography-2.6.1 docker-3.7.2 docker-pycreds-0.4.0 ecdsa-0.13 editdistance-0.5.3 flaky-3.5.3 flask-cors-3.0.7 gevent-1.4.0 jsondiff-1.1.1 jsonnet-0.12.1 moto-1.3.7 msgpack-0.5.6 overrides-1.9 parsimonious-0.8.1 pyaml-18.11.0 pycryptodome-3.8.0 python-jose-2.0.2 pytorch-pretrained-bert-0.6.1 responses-0.10.6 rsa-3.4.2 sqlparse-0.3.0 word2number-1.1 xmltodict-0.12.0\r\n"
     ]
    }
   ],
   "source": [
    "!cp -r ../input/allennlp-packages/packages/packages ./\n",
    "!pip install -r packages/requirements.txt --no-index --find-links packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "FOLD = 0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import glob\n",
    "import gc\n",
    "import logging\n",
    "import requests\n",
    "import re\n",
    "\n",
    "from typing import Dict, Tuple, List\n",
    "from collections import OrderedDict\n",
    "from overrides import overrides\n",
    "from time import sleep\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mlcrate as mlc\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import allennlp\n",
    "\n",
    "from allennlp.common import Registrable, Params\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL, JsonDict\n",
    "\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.fields import ArrayField, TextField\n",
    "from allennlp.data.iterators import BucketIterator, MultiprocessIterator\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, CharacterTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.models import Model\n",
    "\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper # MIGHT USE FOR ABSTRACTION\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "\n",
    "from allennlp.training.metrics import F1Measure, BLEU\n",
    "from allennlp.training import Trainer\n",
    "\n",
    "sys.path.insert(0, './math_handwriting_recognition')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "sample_submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir jigsaw\n",
    "!touch jigsaw/__init__.py\n",
    "\n",
    "# Get a 5 fold cv\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, val_idx = list(kfold.split(train))[0]\n",
    "train_df, val_df = train.iloc[train_idx].reset_index(), train.iloc[val_idx].reset_index()\n",
    "train_df.to_csv('train.csv')\n",
    "val_df.to_csv('val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jigsaw/dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jigsaw/dataset.py\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple, List\n",
    "from overrides import overrides\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import spacy\n",
    "\n",
    "import allennlp\n",
    "\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL, get_spacy_model\n",
    "\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.fields import ArrayField, TextField, MetadataField, LabelField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, CharacterTokenizer, WordTokenizer\n",
    "\n",
    "@Tokenizer.register(\"simple\")\n",
    "class LatexTokenizer(Tokenizer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return [Token(token) for token in text.split()]\n",
    "\n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens = self._tokenize(text)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "@DatasetReader.register('jigsaw')\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, root_path: str, tokenizer: Tokenizer, lazy: bool = True, subset: bool = False) -> None:\n",
    "        super().__init__(lazy)\n",
    "        \n",
    "        self.root_path = root_path\n",
    "        self.subset = subset\n",
    "        \n",
    "        self._tokenizer = tokenizer\n",
    "        self._token_indexer = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file: str):\n",
    "        df = pd.read_csv(os.path.join(self.root_path, file))\n",
    "\n",
    "        if self.subset:\n",
    "            df = df.loc[:16]\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            idx = row['id']\n",
    "            comment_text = row['comment_text']\n",
    "            \n",
    "            if 'target' in df.columns:\n",
    "                target = int(row['target'] > 0.5)\n",
    "                yield self.text_to_instance(idx, comment_text, target)\n",
    "            else:\n",
    "                yield self.text_to_instance(idx, comment_text)\n",
    "            \n",
    "    @overrides\n",
    "    def text_to_instance(self, idx: str, comment_text: str, target: float = None) -> Instance:\n",
    "        comment_text = self._tokenizer.tokenize(comment_text)\n",
    "        \n",
    "        fields = {}\n",
    "        fields['idx'] = MetadataField({'idx': idx})\n",
    "        fields['comment_text'] = TextField(comment_text, self._token_indexer)\n",
    "\n",
    "        if target is not None:\n",
    "            fields['target'] = LabelField(target, skip_indexing=True)\n",
    "        \n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jigsaw/model.py\n"
     ]
    }
   ],
   "source": [
    " %%writefile jigsaw/model.py\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple\n",
    "from overrides import overrides\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import allennlp\n",
    "\n",
    "from allennlp.common import Registrable, Params\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.models import Model\n",
    "\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "\n",
    "from allennlp.training.metrics import F1Measure, BLEU, Auc, BooleanAccuracy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@Model.register('baseline')\n",
    "class Baseline(Model):\n",
    "    def __init__(self, embeddings: TextFieldEmbedder, encoder: Seq2VecEncoder, classifier: FeedForward, vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "\n",
    "        self.embedding = embeddings\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.accuracy = BooleanAccuracy()\n",
    "        \n",
    "    @overrides\n",
    "    def forward(self, idx: Dict[str, torch.Tensor], comment_text: Dict[str, torch.Tensor], target: torch.Tensor = None) -> Dict[str, torch.Tensor]:\n",
    "        mask = get_text_field_mask(comment_text)\n",
    "\n",
    "        x = self.embedding(comment_text)\n",
    "        x = self.encoder(x, mask)\n",
    "        x = self.classifier(x).view(-1)\n",
    "        \n",
    "        logits = torch.sigmoid(x)\n",
    "                \n",
    "        out = {'idx': idx, 'pred': logits}\n",
    "\n",
    "        if target is not None:\n",
    "            if not self.training:\n",
    "                self.accuracy((logits > 0.5).int(), target.int())\n",
    "\n",
    "            out['loss'] = self.loss(x, target.float())\n",
    "\n",
    "        return out\n",
    "\n",
    "    @overrides\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        if not self.training:\n",
    "            metrics = {\n",
    "                \"accuracy\": self.accuracy.get_metric(reset)\n",
    "            }\n",
    "        else:\n",
    "            metrics = {}\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor to get test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jigsaw/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile jigsaw/predictor.py\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple, List\n",
    "from overrides import overrides\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import mlcrate as mlc\n",
    "\n",
    "import allennlp\n",
    "\n",
    "from allennlp.common import Registrable, Params\n",
    "from allennlp.common.util import START_SYMBOL, END_SYMBOL, JsonDict, sanitize\n",
    "\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.models import Model\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "\n",
    "from allennlp.training.metrics import F1Measure, BLEU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@Predictor.register('jigsaw')\n",
    "class JigsawPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        \n",
    "    def dump_line(self, outputs: JsonDict) -> str:\n",
    "        pred = str(outputs['pred'])\n",
    "\n",
    "        return f'{pred}\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config file to set up experiments without changing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"dataset_reader\": {\n",
    "        \"type\": \"jigsaw\",\n",
    "        \"root_path\": \"./\",\n",
    "        \"lazy\": true,\n",
    "        \"subset\": false,\n",
    "        \"tokenizer\": {\n",
    "            \"type\": \"simple\"\n",
    "        }\n",
    "    },\n",
    "    \"train_data_path\": \"train.csv\",\n",
    "    \"validation_data_path\": \"val.csv\",\n",
    "    \"model\": {\n",
    "        \"type\": \"baseline\",\n",
    "        \"embeddings\": {\n",
    "          \"tokens\": {\n",
    "            \"type\": \"embedding\",\n",
    "            \"pretrained_file\": \"../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt\",\n",
    "            \"embedding_dim\": 300,\n",
    "            \"trainable\": false\n",
    "          }\n",
    "        },\n",
    "        'encoder': {\n",
    "            'type': 'lstm',\n",
    "            'bidirectional': false,\n",
    "            'input_size': 300,\n",
    "            'hidden_size': 64,\n",
    "            'num_layers': 1\n",
    "        },\n",
    "        'classifier': {\n",
    "            'input_dim': 64,\n",
    "            'num_layers': 1,\n",
    "            'hidden_dims': 1,\n",
    "            'activations': 'linear' # sigmoid activation is applied separately\n",
    "        }\n",
    "    },\n",
    "    \"iterator\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"sorting_keys\":[[\"comment_text\", \"num_tokens\"]],\n",
    "        \"batch_size\": 512\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"num_epochs\": 4,\n",
    "        \"cuda_device\": 0,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"adam\",\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "        \"grad_clipping\": 5,\n",
    "        \"learning_rate_scheduler\": {\n",
    "            \"type\": \"reduce_on_plateau\",\n",
    "            \"factor\": 0.5,\n",
    "            \"patience\": 5\n",
    "        },\n",
    "        \"num_serialized_models_to_keep\": 1,\n",
    "        \"summary_interval\": 10,\n",
    "        \"histogram_interval\": 100,\n",
    "        \"should_log_parameter_statistics\": true,\n",
    "        \"should_log_learning_rate\": true\n",
    "    },\n",
    "    'vocabulary': {\n",
    "        'max_vocab_size': 100000,\n",
    "#         \"directory_path\": \"./vocabulary\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\n",
      "2019-04-06 20:45:08,618 - INFO - allennlp.common.params - random_seed = 13370\r\n",
      "2019-04-06 20:45:08,618 - INFO - allennlp.common.params - numpy_seed = 1337\r\n",
      "2019-04-06 20:45:08,618 - INFO - allennlp.common.params - pytorch_seed = 133\r\n",
      "2019-04-06 20:45:08,619 - INFO - allennlp.common.checks - Pytorch version: 1.0.1.post2\r\n",
      "2019-04-06 20:45:08,620 - INFO - allennlp.common.params - evaluate_on_test = False\r\n",
      "2019-04-06 20:45:08,620 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}, 'type': 'jigsaw'} and extras set()\r\n",
      "2019-04-06 20:45:08,621 - INFO - allennlp.common.params - dataset_reader.type = jigsaw\r\n",
      "2019-04-06 20:45:08,621 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.JigsawDatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}} and extras set()\r\n",
      "2019-04-06 20:45:08,621 - INFO - allennlp.common.params - dataset_reader.root_path = ./\r\n",
      "2019-04-06 20:45:08,621 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'simple'} and extras set()\r\n",
      "2019-04-06 20:45:08,621 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = simple\r\n",
      "2019-04-06 20:45:08,621 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.LatexTokenizer'> from params {} and extras set()\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - dataset_reader.lazy = True\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - dataset_reader.subset = False\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - validation_dataset_reader = None\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - train_data_path = train.csv\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.training.util - Reading training data from train.csv\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - validation_data_path = val.csv\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.training.util - Reading validation data from val.csv\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - test_data_path = None\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.training.trainer - From dataset instances, train, validation will be considered for vocabulary creation.\r\n",
      "2019-04-06 20:45:08,622 - INFO - allennlp.common.params - vocabulary.type = None\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.extend = False\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.directory_path = None\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.min_count = None\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.max_vocab_size = 100000\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None\r\n",
      "2019-04-06 20:45:08,623 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.\r\n",
      "1804874it [06:47, 4424.94it/s]\r\n",
      "2019-04-06 20:51:57,458 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'pretrained_file': '../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt', 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}, 'type': 'baseline'} and extras {'vocab'}\r\n",
      "2019-04-06 20:51:57,459 - INFO - allennlp.common.params - model.type = baseline\r\n",
      "2019-04-06 20:51:57,459 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.model.Baseline'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'pretrained_file': '../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt', 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}} and extras {'vocab'}\r\n",
      "2019-04-06 20:51:57,459 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'pretrained_file': '../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt', 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\r\n",
      "2019-04-06 20:51:57,459 - INFO - allennlp.common.params - model.embeddings.type = basic\r\n",
      "2019-04-06 20:51:57,460 - INFO - allennlp.common.params - model.embeddings.embedder_to_indexer_map = None\r\n",
      "2019-04-06 20:51:57,460 - INFO - allennlp.common.params - model.embeddings.allow_unmatched_keys = False\r\n",
      "2019-04-06 20:51:57,460 - INFO - allennlp.common.params - model.embeddings.token_embedders = None\r\n",
      "2019-04-06 20:51:57,460 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': '../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt', 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\r\n",
      "2019-04-06 20:51:57,460 - INFO - allennlp.common.params - model.embeddings.tokens.type = embedding\r\n",
      "2019-04-06 20:51:57,460 - INFO - allennlp.common.params - model.embeddings.tokens.num_embeddings = None\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.vocab_namespace = tokens\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.embedding_dim = 300\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.pretrained_file = ../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.projection_dim = None\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.trainable = False\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.padding_index = None\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.max_norm = None\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.norm_type = 2.0\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.scale_grad_by_freq = False\r\n",
      "2019-04-06 20:51:57,461 - INFO - allennlp.common.params - model.embeddings.tokens.sparse = False\r\n",
      "2019-04-06 20:51:57,479 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file\r\n",
      "2196017it [00:13, 159852.87it/s]\r\n",
      "2019-04-06 20:52:11,315 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer\r\n",
      "2019-04-06 20:52:12,107 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 53433 out of 100002 tokens\r\n",
      "2019-04-06 20:52:12,129 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\r\n",
      "2019-04-06 20:52:12,129 - INFO - allennlp.common.params - model.encoder.type = lstm\r\n",
      "2019-04-06 20:52:12,129 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - model.encoder.bidirectional = False\r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - model.encoder.hidden_size = 64\r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - model.encoder.input_size = 300\r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - model.encoder.num_layers = 1\r\n",
      "2019-04-06 20:52:12,130 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 20:52:12,135 - INFO - allennlp.common.params - model.classifier.input_dim = 64\r\n",
      "2019-04-06 20:52:12,135 - INFO - allennlp.common.params - model.classifier.num_layers = 1\r\n",
      "2019-04-06 20:52:12,135 - INFO - allennlp.common.params - model.classifier.hidden_dims = 1\r\n",
      "2019-04-06 20:52:12,135 - INFO - allennlp.common.params - model.classifier.activations = linear\r\n",
      "2019-04-06 20:52:12,135 - INFO - allennlp.common.params - model.classifier.dropout = 0.0\r\n",
      "2019-04-06 20:52:12,416 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 512, 'sorting_keys': [['comment_text', 'num_tokens']], 'type': 'bucket'} and extras set()\r\n",
      "2019-04-06 20:52:12,416 - INFO - allennlp.common.params - iterator.type = bucket\r\n",
      "2019-04-06 20:52:12,416 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 512, 'sorting_keys': [['comment_text', 'num_tokens']]} and extras set()\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.sorting_keys = [['comment_text', 'num_tokens']]\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.padding_noise = 0.1\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.biggest_batch_first = False\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.batch_size = 512\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.instances_per_epoch = None\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.cache_instances = False\r\n",
      "2019-04-06 20:52:12,417 - INFO - allennlp.common.params - iterator.track_epoch = False\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.common.params - validation_iterator = None\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.common.params - trainer.no_grad = ()\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - Following parameters are Frozen  (without gradient):\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - embedding.token_embedder_tokens.weight\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - Following parameters are Tunable (with gradient):\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - encoder._module.weight_ih_l0\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - encoder._module.weight_hh_l0\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - encoder._module.bias_ih_l0\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - encoder._module.bias_hh_l0\r\n",
      "2019-04-06 20:52:12,418 - INFO - allennlp.training.trainer - classifier._linear_layers.0.weight\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.training.trainer - classifier._linear_layers.0.bias\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.patience = None\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.validation_metric = -loss\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.shuffle = True\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.num_epochs = 4\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.cuda_device = 0\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.grad_norm = None\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.grad_clipping = 5\r\n",
      "2019-04-06 20:52:12,419 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\r\n",
      "2019-04-06 20:52:16,328 - INFO - allennlp.common.params - trainer.optimizer.type = adam\r\n",
      "2019-04-06 20:52:16,329 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None\r\n",
      "2019-04-06 20:52:16,329 - INFO - allennlp.training.optimizers - Number of trainable parameters: 93761\r\n",
      "2019-04-06 20:52:16,329 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True\r\n",
      "2019-04-06 20:52:16,329 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\r\n",
      "2019-04-06 20:52:16,329 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \r\n",
      "2019-04-06 20:52:16,329 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.001\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = reduce_on_plateau\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.factor = 0.5\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.patience = 5\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.model_save_interval = None\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.summary_interval = 10\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.histogram_interval = 100\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.should_log_learning_rate = True\r\n",
      "2019-04-06 20:52:16,330 - INFO - allennlp.common.params - trainer.log_batch_size_period = None\r\n",
      "2019-04-06 20:52:16,331 - WARNING - allennlp.training.trainer - You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\r\n",
      "2019-04-06 20:52:16,438 - INFO - allennlp.training.trainer - Beginning training.\r\n",
      "2019-04-06 20:52:16,438 - INFO - allennlp.training.trainer - Epoch 0/3\r\n",
      "2019-04-06 20:52:16,438 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 4200.192\r\n",
      "2019-04-06 20:52:16,528 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 761\r\n",
      "2019-04-06 20:52:16,529 - INFO - allennlp.training.trainer - Training\r\n",
      "loss: 0.1507 ||: : 2821it [15:33,  2.05it/s]\r\n",
      "2019-04-06 21:07:49,837 - INFO - allennlp.training.trainer - Validating\r\n",
      "accuracy: 0.9572, loss: 0.1284 ||: : 576it [02:39,  3.63it/s]"
     ]
    }
   ],
   "source": [
    "!allennlp train config.json -s ./logs --include-package jigsaw\n",
    "# !rm -rf logs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model's performance on the train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\n",
      "2019-04-06 22:07:42,342 - INFO - allennlp.models.archival - loading archive file ./logs/model.tar.gz\r\n",
      "2019-04-06 22:07:42,343 - INFO - allennlp.models.archival - extracting archive file ./logs/model.tar.gz to temp dir /tmp/tmpfk7rmpiw\r\n",
      "2019-04-06 22:07:43,386 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpfk7rmpiw/vocabulary.\r\n",
      "2019-04-06 22:07:43,484 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}, 'type': 'baseline'} and extras {'vocab'}\r\n",
      "2019-04-06 22:07:43,485 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.model.Baseline'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:07:43,485 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:07:43,485 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\r\n",
      "2019-04-06 22:07:43,736 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\r\n",
      "2019-04-06 22:07:46,175 - INFO - allennlp.common.checks - Pytorch version: 1.0.1.post2\r\n",
      "2019-04-06 22:07:46,175 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}, 'type': 'jigsaw'} and extras set()\r\n",
      "2019-04-06 22:07:46,176 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.JigsawDatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}} and extras set()\r\n",
      "2019-04-06 22:07:46,176 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'simple'} and extras set()\r\n",
      "2019-04-06 22:07:46,176 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.LatexTokenizer'> from params {} and extras set()\r\n",
      "2019-04-06 22:07:46,176 - INFO - allennlp.commands.evaluate - Reading evaluation data from train.csv\r\n",
      "2019-04-06 22:07:46,176 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 512, 'sorting_keys': [['comment_text', 'num_tokens']], 'type': 'bucket'} and extras set()\r\n",
      "2019-04-06 22:07:46,177 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 512, 'sorting_keys': [['comment_text', 'num_tokens']]} and extras set()\r\n",
      "2019-04-06 22:07:46,177 - INFO - allennlp.training.util - Iterating over dataset\r\n",
      "accuracy: 0.96, loss: 0.10 ||: : 2821it [13:16,  2.28it/s]\r\n",
      "2019-04-06 22:21:02,716 - INFO - allennlp.commands.evaluate - Finished evaluating.\r\n",
      "2019-04-06 22:21:02,716 - INFO - allennlp.commands.evaluate - Metrics:\r\n",
      "2019-04-06 22:21:02,716 - INFO - allennlp.commands.evaluate - accuracy: 0.9637931738992824\r\n",
      "2019-04-06 22:21:02,716 - INFO - allennlp.commands.evaluate - loss: 0.10184237466598817\r\n",
      "2019-04-06 22:21:02,720 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpfk7rmpiw\r\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\n",
      "2019-04-06 22:21:07,134 - INFO - allennlp.models.archival - loading archive file ./logs/model.tar.gz\r\n",
      "2019-04-06 22:21:07,135 - INFO - allennlp.models.archival - extracting archive file ./logs/model.tar.gz to temp dir /tmp/tmpn61xdqiz\r\n",
      "2019-04-06 22:21:08,217 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpn61xdqiz/vocabulary.\r\n",
      "2019-04-06 22:21:08,312 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}, 'type': 'baseline'} and extras {'vocab'}\r\n",
      "2019-04-06 22:21:08,313 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.model.Baseline'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:21:08,313 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:21:08,314 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\r\n",
      "2019-04-06 22:21:08,567 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\r\n",
      "2019-04-06 22:21:11,039 - INFO - allennlp.common.checks - Pytorch version: 1.0.1.post2\r\n",
      "2019-04-06 22:21:11,039 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}, 'type': 'jigsaw'} and extras set()\r\n",
      "2019-04-06 22:21:11,040 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.JigsawDatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}} and extras set()\r\n",
      "2019-04-06 22:21:11,040 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'simple'} and extras set()\r\n",
      "2019-04-06 22:21:11,040 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.LatexTokenizer'> from params {} and extras set()\r\n",
      "2019-04-06 22:21:11,040 - INFO - allennlp.commands.evaluate - Reading evaluation data from val.csv\r\n",
      "2019-04-06 22:21:11,040 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 512, 'sorting_keys': [['comment_text', 'num_tokens']], 'type': 'bucket'} and extras set()\r\n",
      "2019-04-06 22:21:11,041 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 512, 'sorting_keys': [['comment_text', 'num_tokens']]} and extras set()\r\n",
      "2019-04-06 22:21:11,041 - INFO - allennlp.training.util - Iterating over dataset\r\n",
      "accuracy: 0.96, loss: 0.11 ||: : 706it [03:11,  4.20it/s]\r\n",
      "2019-04-06 22:24:22,317 - INFO - allennlp.commands.evaluate - Finished evaluating.\r\n",
      "2019-04-06 22:24:22,317 - INFO - allennlp.commands.evaluate - Metrics:\r\n",
      "2019-04-06 22:24:22,317 - INFO - allennlp.commands.evaluate - accuracy: 0.9607618256111919\r\n",
      "2019-04-06 22:24:22,317 - INFO - allennlp.commands.evaluate - loss: 0.11243568266881762\r\n",
      "2019-04-06 22:24:22,321 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpn61xdqiz\r\n"
     ]
    }
   ],
   "source": [
    "!allennlp evaluate --cuda-device 0 --include-package jigsaw ./logs/model.tar.gz train.csv\n",
    "!allennlp evaluate --cuda-device 0 --include-package jigsaw ./logs/model.tar.gz val.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\n",
      "2019-04-06 22:24:28,401 - INFO - allennlp.models.archival - loading archive file ./logs/model.tar.gz\r\n",
      "2019-04-06 22:24:28,402 - INFO - allennlp.models.archival - extracting archive file ./logs/model.tar.gz to temp dir /tmp/tmp720urff3\r\n",
      "2019-04-06 22:24:29,448 - INFO - allennlp.common.params - vocabulary.type = default\r\n",
      "2019-04-06 22:24:29,448 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmp720urff3/vocabulary.\r\n",
      "2019-04-06 22:24:29,545 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}, 'type': 'baseline'} and extras {'vocab'}\r\n",
      "2019-04-06 22:24:29,546 - INFO - allennlp.common.params - model.type = baseline\r\n",
      "2019-04-06 22:24:29,546 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.model.Baseline'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:24:29,546 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.type = basic\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.embedder_to_indexer_map = None\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.allow_unmatched_keys = False\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.token_embedders = None\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.tokens.type = embedding\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.tokens.num_embeddings = None\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.tokens.vocab_namespace = tokens\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.tokens.embedding_dim = 300\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.tokens.pretrained_file = None\r\n",
      "2019-04-06 22:24:29,547 - INFO - allennlp.common.params - model.embeddings.tokens.projection_dim = None\r\n",
      "2019-04-06 22:24:29,548 - INFO - allennlp.common.params - model.embeddings.tokens.trainable = False\r\n",
      "2019-04-06 22:24:29,548 - INFO - allennlp.common.params - model.embeddings.tokens.padding_index = None\r\n",
      "2019-04-06 22:24:29,548 - INFO - allennlp.common.params - model.embeddings.tokens.max_norm = None\r\n",
      "2019-04-06 22:24:29,548 - INFO - allennlp.common.params - model.embeddings.tokens.norm_type = 2.0\r\n",
      "2019-04-06 22:24:29,548 - INFO - allennlp.common.params - model.embeddings.tokens.scale_grad_by_freq = False\r\n",
      "2019-04-06 22:24:29,548 - INFO - allennlp.common.params - model.embeddings.tokens.sparse = False\r\n",
      "2019-04-06 22:24:29,801 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.type = lstm\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.bidirectional = False\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.hidden_size = 64\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.input_size = 300\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.num_layers = 1\r\n",
      "2019-04-06 22:24:29,802 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 22:24:29,804 - INFO - allennlp.common.params - model.classifier.input_dim = 64\r\n",
      "2019-04-06 22:24:29,804 - INFO - allennlp.common.params - model.classifier.num_layers = 1\r\n",
      "2019-04-06 22:24:29,804 - INFO - allennlp.common.params - model.classifier.hidden_dims = 1\r\n",
      "2019-04-06 22:24:29,804 - INFO - allennlp.common.params - model.classifier.activations = linear\r\n",
      "2019-04-06 22:24:29,804 - INFO - allennlp.common.params - model.classifier.dropout = 0.0\r\n",
      "2019-04-06 22:24:32,387 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}, 'type': 'jigsaw'} and extras set()\r\n",
      "2019-04-06 22:24:32,387 - INFO - allennlp.common.params - dataset_reader.type = jigsaw\r\n",
      "2019-04-06 22:24:32,387 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.JigsawDatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}} and extras set()\r\n",
      "2019-04-06 22:24:32,387 - INFO - allennlp.common.params - dataset_reader.root_path = ./\r\n",
      "2019-04-06 22:24:32,388 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'simple'} and extras set()\r\n",
      "2019-04-06 22:24:32,388 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = simple\r\n",
      "2019-04-06 22:24:32,388 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.LatexTokenizer'> from params {} and extras set()\r\n",
      "2019-04-06 22:24:32,388 - INFO - allennlp.common.params - dataset_reader.lazy = True\r\n",
      "2019-04-06 22:24:32,388 - INFO - allennlp.common.params - dataset_reader.subset = False\r\n",
      "2019-04-06 22:24:46,263 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\r\n",
      "2019-04-06 22:40:05,153 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp720urff3\r\n",
      "CPU times: user 14.8 s, sys: 3.4 s, total: 18.2 s\n",
      "Wall time: 15min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!allennlp predict --output-file ./train_preds.csv --batch-size 64 --cuda-device 0 --use-dataset-reader --predictor jigsaw --include-package jigsaw --silent ./logs/model.tar.gz train.csv\n",
    "# From https://superuser.com/questions/246837/how-do-i-add-text-to-the-beginning-of-a-file-in-bash\n",
    "!sed -i '1s/^/prediction\\n/' train_preds.csv\n",
    "train_preds = pd.read_csv('train_preds.csv')\n",
    "train_roc_auc_score = roc_auc_score(train_df.target.values > 0.5, train_preds.prediction.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\n",
      "2019-04-06 22:40:13,555 - INFO - allennlp.models.archival - loading archive file ./logs/model.tar.gz\r\n",
      "2019-04-06 22:40:13,556 - INFO - allennlp.models.archival - extracting archive file ./logs/model.tar.gz to temp dir /tmp/tmpiw2mry9z\r\n",
      "2019-04-06 22:40:14,773 - INFO - allennlp.common.params - vocabulary.type = default\r\n",
      "2019-04-06 22:40:14,774 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpiw2mry9z/vocabulary.\r\n",
      "2019-04-06 22:40:14,888 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}, 'type': 'baseline'} and extras {'vocab'}\r\n",
      "2019-04-06 22:40:14,889 - INFO - allennlp.common.params - model.type = baseline\r\n",
      "2019-04-06 22:40:14,889 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.model.Baseline'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.params - model.embeddings.type = basic\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.params - model.embeddings.embedder_to_indexer_map = None\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.params - model.embeddings.allow_unmatched_keys = False\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.params - model.embeddings.token_embedders = None\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.params - model.embeddings.tokens.type = embedding\r\n",
      "2019-04-06 22:40:14,890 - INFO - allennlp.common.params - model.embeddings.tokens.num_embeddings = None\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.vocab_namespace = tokens\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.embedding_dim = 300\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.pretrained_file = None\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.projection_dim = None\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.trainable = False\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.padding_index = None\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.max_norm = None\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.norm_type = 2.0\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.scale_grad_by_freq = False\r\n",
      "2019-04-06 22:40:14,891 - INFO - allennlp.common.params - model.embeddings.tokens.sparse = False\r\n",
      "2019-04-06 22:40:15,193 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\r\n",
      "2019-04-06 22:40:15,193 - INFO - allennlp.common.params - model.encoder.type = lstm\r\n",
      "2019-04-06 22:40:15,193 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 22:40:15,193 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\r\n",
      "2019-04-06 22:40:15,193 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \r\n",
      "2019-04-06 22:40:15,193 - INFO - allennlp.common.params - model.encoder.bidirectional = False\r\n",
      "2019-04-06 22:40:15,194 - INFO - allennlp.common.params - model.encoder.hidden_size = 64\r\n",
      "2019-04-06 22:40:15,194 - INFO - allennlp.common.params - model.encoder.input_size = 300\r\n",
      "2019-04-06 22:40:15,194 - INFO - allennlp.common.params - model.encoder.num_layers = 1\r\n",
      "2019-04-06 22:40:15,194 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 22:40:15,195 - INFO - allennlp.common.params - model.classifier.input_dim = 64\r\n",
      "2019-04-06 22:40:15,195 - INFO - allennlp.common.params - model.classifier.num_layers = 1\r\n",
      "2019-04-06 22:40:15,196 - INFO - allennlp.common.params - model.classifier.hidden_dims = 1\r\n",
      "2019-04-06 22:40:15,196 - INFO - allennlp.common.params - model.classifier.activations = linear\r\n",
      "2019-04-06 22:40:15,196 - INFO - allennlp.common.params - model.classifier.dropout = 0.0\r\n",
      "2019-04-06 22:40:18,001 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}, 'type': 'jigsaw'} and extras set()\r\n",
      "2019-04-06 22:40:18,001 - INFO - allennlp.common.params - dataset_reader.type = jigsaw\r\n",
      "2019-04-06 22:40:18,001 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.JigsawDatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}} and extras set()\r\n",
      "2019-04-06 22:40:18,002 - INFO - allennlp.common.params - dataset_reader.root_path = ./\r\n",
      "2019-04-06 22:40:18,002 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'simple'} and extras set()\r\n",
      "2019-04-06 22:40:18,002 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = simple\r\n",
      "2019-04-06 22:40:18,002 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.LatexTokenizer'> from params {} and extras set()\r\n",
      "2019-04-06 22:40:18,002 - INFO - allennlp.common.params - dataset_reader.lazy = True\r\n",
      "2019-04-06 22:40:18,003 - INFO - allennlp.common.params - dataset_reader.subset = False\r\n",
      "2019-04-06 22:40:21,654 - WARNING - allennlp.models.model - Encountered the loss key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\r\n",
      "2019-04-06 22:44:03,528 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpiw2mry9z\r\n",
      "CPU times: user 3.68 s, sys: 996 ms, total: 4.68 s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!allennlp predict --output-file ./val_preds.csv --batch-size 64 --cuda-device 0 --use-dataset-reader --predictor jigsaw --include-package jigsaw --silent ./logs/model.tar.gz val.csv\n",
    "# From https://superuser.com/questions/246837/how-do-i-add-text-to-the-beginning-of-a-file-in-bash\n",
    "!sed -i '1s/^/prediction\\n/' val_preds.csv\n",
    "val_preds = pd.read_csv('val_preds.csv')\n",
    "val_roc_auc_score = roc_auc_score(val_df.target.values > 0.5, val_preds.prediction.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"best_epoch\": 3,\r\n",
      "  \"peak_cpu_memory_MB\": 6563.652,\r\n",
      "  \"peak_gpu_0_memory_MB\": 1329,\r\n",
      "  \"training_duration\": \"01:15:04\",\r\n",
      "  \"training_start_epoch\": 0,\r\n",
      "  \"training_epochs\": 3,\r\n",
      "  \"epoch\": 3,\r\n",
      "  \"training_loss\": 0.1069820237150221,\r\n",
      "  \"training_cpu_memory_MB\": 6563.652,\r\n",
      "  \"training_gpu_0_memory_MB\": 1329,\r\n",
      "  \"validation_accuracy\": 0.9607618256111919,\r\n",
      "  \"validation_loss\": 0.11243568266354101,\r\n",
      "  \"best_validation_accuracy\": 0.9607618256111919,\r\n",
      "  \"best_validation_loss\": 0.11243568266354101\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat logs/metrics.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ROC-AUC: 0.9543\n",
      "Val ROC-AUC: 0.9415\n"
     ]
    }
   ],
   "source": [
    "print(f'Train ROC-AUC: {round(train_roc_auc_score, 4)}')\n",
    "print(f'Val ROC-AUC: {round(val_roc_auc_score, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set and save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\n",
      "2019-04-06 22:44:10,079 - INFO - allennlp.models.archival - loading archive file ./logs/model.tar.gz\r\n",
      "2019-04-06 22:44:10,080 - INFO - allennlp.models.archival - extracting archive file ./logs/model.tar.gz to temp dir /tmp/tmpj_zryeci\r\n",
      "2019-04-06 22:44:11,118 - INFO - allennlp.common.params - vocabulary.type = default\r\n",
      "2019-04-06 22:44:11,118 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmpj_zryeci/vocabulary.\r\n",
      "2019-04-06 22:44:11,214 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}, 'type': 'baseline'} and extras {'vocab'}\r\n",
      "2019-04-06 22:44:11,214 - INFO - allennlp.common.params - model.type = baseline\r\n",
      "2019-04-06 22:44:11,214 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.model.Baseline'> from params {'classifier': {'activations': 'linear', 'hidden_dims': 1, 'input_dim': 64, 'num_layers': 1}, 'embeddings': {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}}, 'encoder': {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:44:11,214 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'}} and extras {'vocab'}\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.type = basic\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.embedder_to_indexer_map = None\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.allow_unmatched_keys = False\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.token_embedders = None\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'trainable': False, 'type': 'embedding'} and extras {'vocab'}\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.tokens.type = embedding\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.tokens.num_embeddings = None\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.tokens.vocab_namespace = tokens\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.tokens.embedding_dim = 300\r\n",
      "2019-04-06 22:44:11,215 - INFO - allennlp.common.params - model.embeddings.tokens.pretrained_file = None\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.projection_dim = None\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.trainable = False\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.padding_index = None\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.max_norm = None\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.norm_type = 2.0\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.scale_grad_by_freq = False\r\n",
      "2019-04-06 22:44:11,216 - INFO - allennlp.common.params - model.embeddings.tokens.sparse = False\r\n",
      "2019-04-06 22:44:11,470 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'bidirectional': False, 'hidden_size': 64, 'input_size': 300, 'num_layers': 1, 'type': 'lstm'} and extras {'vocab'}\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.type = lstm\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: \r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.bidirectional = False\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.hidden_size = 64\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.input_size = 300\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.num_layers = 1\r\n",
      "2019-04-06 22:44:11,471 - INFO - allennlp.common.params - model.encoder.batch_first = True\r\n",
      "2019-04-06 22:44:11,473 - INFO - allennlp.common.params - model.classifier.input_dim = 64\r\n",
      "2019-04-06 22:44:11,473 - INFO - allennlp.common.params - model.classifier.num_layers = 1\r\n",
      "2019-04-06 22:44:11,473 - INFO - allennlp.common.params - model.classifier.hidden_dims = 1\r\n",
      "2019-04-06 22:44:11,473 - INFO - allennlp.common.params - model.classifier.activations = linear\r\n",
      "2019-04-06 22:44:11,473 - INFO - allennlp.common.params - model.classifier.dropout = 0.0\r\n",
      "2019-04-06 22:44:14,236 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}, 'type': 'jigsaw'} and extras set()\r\n",
      "2019-04-06 22:44:14,236 - INFO - allennlp.common.params - dataset_reader.type = jigsaw\r\n",
      "2019-04-06 22:44:14,237 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.JigsawDatasetReader'> from params {'lazy': True, 'root_path': './', 'subset': False, 'tokenizer': {'type': 'simple'}} and extras set()\r\n",
      "2019-04-06 22:44:14,237 - INFO - allennlp.common.params - dataset_reader.root_path = ./\r\n",
      "2019-04-06 22:44:14,237 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'simple'} and extras set()\r\n",
      "2019-04-06 22:44:14,237 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = simple\r\n",
      "2019-04-06 22:44:14,237 - INFO - allennlp.common.from_params - instantiating class <class 'jigsaw.dataset.LatexTokenizer'> from params {} and extras set()\r\n",
      "2019-04-06 22:44:14,237 - INFO - allennlp.common.params - dataset_reader.lazy = True\r\n",
      "2019-04-06 22:44:14,238 - INFO - allennlp.common.params - dataset_reader.subset = False\r\n",
      "2019-04-06 22:45:10,485 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmpj_zryeci\r\n",
      "CPU times: user 1.32 s, sys: 364 ms, total: 1.68 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!allennlp predict --output-file ./test_preds.csv --batch-size 64 --cuda-device 0 --use-dataset-reader --predictor jigsaw --include-package jigsaw --silent ./logs/model.tar.gz ../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv\n",
    "# From https://superuser.com/questions/246837/how-do-i-add-text-to-the-beginning-of-a-file-in-bash\n",
    "!sed -i '1s/^/prediction\\n/' test_preds.csv\n",
    "test_preds = pd.read_csv('test_preds.csv')\n",
    "sample_submission['prediction'] = test_preds['prediction'].values\n",
    "mlc.kaggle.save_sub(sample_submission, 'submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.007651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.001997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.961583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.007651\n",
       "1  7000001    0.000628\n",
       "2  7000002    0.001997\n",
       "3  7000003    0.000672\n",
       "4  7000004    0.961583"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete unnecessary files to free up more space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'out.txt': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf logs\n",
    "!rm out.txt\n",
    "!rm config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
