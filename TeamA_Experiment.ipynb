{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experement Presentation by Team_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- surmarize a normal passage a combination of extractive and abstractive techniques then after apply it on googles T5 text summarizer\n",
    "\n",
    "- adjust text leanght and observe the outcomes\n",
    "\n",
    "- apply the above  on a data frame\n",
    "\n",
    "- demonstrate a question and answer Model\n",
    "\n",
    "- Apply the question and answer model on a data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1  Setting up necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "#models\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from summarizer import Summarizer\n",
    "#predictor = Predictor.from_path(\"C:/Users/Silas_Dell/Downloads/Compressed/bidaf-elmo.2021-02-11.tar_2.gz\")\n",
    "\n",
    "\n",
    "from allennlp.predictors import Predictor\n",
    "from allennlp_models.pretrained import load_predictor\n",
    "\n",
    "# Suppressing unnwarranted warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and Tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-context",
   "metadata": {},
   "source": [
    "### Input Text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "New York (CNN Business)Netflix is synonymous with streaming, but its competitors have a distinct advantage that threatens the streaming leader's position at the top.\n",
    "Disney has Disney+, but it also has theme parks, plush Baby Yoda dolls, blockbuster Marvel movies and ESPN. Comcast (CMCSA), Amazon (AMZN), ViacomCBS (VIACA), CNN's parent company WarnerMedia and Apple (AAPL) all have their own streaming services, too, but they also have other forms of revenue.\n",
    "As for Netflix (NFLX), its revenue driver is based entirely on building its subscriber base. It's worked out well for the company - so far. But it's starting to look like the king of streaming will soon need something other than new subscribers to keep growing.\n",
    "The streaming service reported Tuesday it now has 208 million subscribers globally, after adding 4 million subscribers in the first quarter of 2021. But that number missed expectations and the forecasts for its next quarter were also pretty weak.\n",
    "That was a big whiff for Netflix - a company coming off a massive year of growth thanks in large part to the pandemic driving people indoors - and Wall Street's reaction has not been great.\n",
    "The company's stock dropped as much as 8% on Wednesday, leading some to wonder what the future of the streamer looks like if competition continues to gain strength, people start heading outdoors and if, most importantly, its growth slows.\n",
    "\"If you hit a wall with [subscriptions] then you pretty much don't have a super growth strategy anymore in your most developed markets,\" Michael Nathanson, a media analyst and founding partner at MoffettNathanson, told CNN Business. \"What can they do to take even more revenue out of the market, above and beyond streaming revenues?\"\n",
    "Or put another way, the company's lackluster user growth last quarter is a signal that it wouldn't hurt if Netflix - a company that's lived and died with its subscriber numbers - started thinking about other ways to make money.\n",
    "An ad-supported Netflix? Not so fast\n",
    "There are ways for Netflix to make money other than raising prices or adding subscribers. The most obvious: selling advertising.\n",
    "Netflix could have 30-second commercials on their programming or get sponsors for their biggest series and films. TV has worked that way forever, why not Netflix?\n",
    "That's probably not going to happen, given that CEO Reed Hastings has been vocal about the unlikelihood of an ad-supported Netflix service. His reasoning: It doesn't make business sense.\n",
    "\"It's a judgment call... It's a belief we can build a better business, a more valuable business [without advertising],\" Hastings told Variety in September. \"You know, advertising looks easy until you get in it. Then you realize you have to rip that revenue away from other places because the total ad market isn't growing, and in fact right now it's shrinking. It's hand-to-hand combat to get people to spend less on, you know, ABC and to spend more on Netflix.\"\n",
    "Hastings added that \"there's much more growth in the consumer market than there is in advertising, which is pretty flat.\"\n",
    "He's also expressed doubts about Netflix getting into live sports or news, which could boost the service's allure to subscribers, so that's likely out, too, at least for now.\n",
    "So if Netflix is looking for other forms of near-term revenue to help support its hefty content budget ($17 billion in 2021 alone) then what can it do? There is one place that could be a revenue driver for Netflix, but if you're borrowing your mother's account you won't like it.\n",
    "Netflix could crack down on password sharing - a move that the company has been considering lately.\n",
    "\"Basically you're going to clean up some subscribers that are free riders,\" Nathanson said. \"That's going to help them get to a higher level of penetration, definitely, but not in long-term.\"\n",
    "Lackluster growth is still growth\n",
    "Missing projections is never good, but it's hardly the end of the world for Netflix. The company remains the market leader and most competitors are still far from taking the company on. And while Netflix's first-quarter subscriber growth wasn't great, and its forecasts for the next quarter alarmed investors, it was just one quarter.\n",
    "Netflix has had subscriber misses before and it's still the most dominant name in all of streaming, and even lackluster growth is still growth. It's not as if people are canceling Netflix in droves.\n",
    "Asked about Netflix's \"second act\" during the company's post-earnings call on Tuesday, Hastings again placed the company's focus on pleasing subscribers.\n",
    "\"We do want to expand. We used to do that thing shipping DVDs, and luckily we didn't get stuck with that. We didn't define that as the main thing. We define entertainment as the main thing,\" Hastings said.\n",
    "He added that he doesn't think Netflix will have a second act in the way Amazon has had with Amazon shopping and Amazon Web Services. Rather, Netflix will continue to improve and grow on what it already does best.\n",
    "\"I'll bet we end with one hopefully gigantic, hopefully defensible profit pool, and continue to improve the service for our members,\" he said. \"I wouldn't look for any large secondary pool of profits. There will be a bunch of supporting pools, like consumer products, that can be both profitable and can support the title brands.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-contact",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input = tokenizer.encode(\"summarize: \"+text, return_tensors='pt', \n",
    "                                max_length=tokenizer.model_max_length, \n",
    "                                truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokens_input, min_length=80,\n",
    "                             max_length=150,\n",
    "                             length_penalty=20, \n",
    "                             num_beams=2)\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens generated from the text using T5 Tokenizer\n",
    "len(tokenizer(text)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-amount",
   "metadata": {},
   "source": [
    "Using BERT summarizer to extract only top 50% of sentences that are considered important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = Summarizer()\n",
    "ext_summary = bert_model(text, ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ext_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-mitchell",
   "metadata": {},
   "source": [
    "## Tokenize BERT Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input_2 = tokenizer.encode(\"summarize: \"+ext_summary, return_tensors='pt', \n",
    "                                max_length=tokenizer.model_max_length, \n",
    "                                truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-formation",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer(ext_summary)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-webmaster",
   "metadata": {},
   "source": [
    "## Extractive-Abstractive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids_2 = model.generate(tokens_input_2, min_length=80,\n",
    "                             max_length=150,\n",
    "                             length_penalty=20, \n",
    "                             num_beams=2)\n",
    "\n",
    "summary_2 = tokenizer.decode(summary_ids_2[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "AMONG the numerous mechanisms that make human life possible is the body’s ability to heal wounds and regenerate damaged tissue. The process begins as soon as an injury occurs.\n",
    "Consider: The healing process is made possible by a cascade of complex cellular functions:\n",
    "Platelets adhere to tissues around a wound, forming a blood clot and sealing damaged blood vessels.\n",
    "Inflammation protects against infection and removes any “debris” caused by the injury.\n",
    "Within days, the body begins to replace injured tissue, make the wound contract, and repair damaged blood vessels.\n",
    "Finally, scar tissue remodels and strengthens the damaged area.\n",
    "Inspired by blood clotting, researchers are developing plastics that can “heal” \n",
    "damage to themselves. Such regenerating materials are equipped with tiny parallel \n",
    "tubes containing two chemicals that “bleed” when any damage occurs. As the two \n",
    "chemicals mix, they form a gel that spreads across the damaged areas, closing \n",
    "cracks and holes. As the gel solidifies, it forms a tough substance that restores \n",
    "the material’s original strength. One researcher admits that this synthetic healing process currently under development is “reminiscent” of what already exists in nature.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_input = tokenizer.encode(\"summarize: \"+text1, return_tensors='pt', \n",
    "                                max_length=tokenizer.model_max_length, \n",
    "                                truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokens_input, min_length=80,\n",
    "                             max_length=150,\n",
    "                             length_penalty=20, \n",
    "                             num_beams=2)\n",
    "\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens generated from the text using T5 Tokenizer\n",
    "len(tokenizer(text)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('guardian_publications.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Selecting 5 rows to work on\n",
    "# train = train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(train[\"text\"])):\n",
    "    result.append(tokenizer.encode(\"summarize: \"+train[\"text\"][i], return_tensors='pt', \n",
    "                                max_length=tokenizer.model_max_length, \n",
    "                                truncation=True))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tokens_input\"]=result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = []\n",
    "for i in range(len(train[\"tokens_input\"])):\n",
    "    result1.append(model.generate(train[\"tokens_input\"][i], min_length=80,\n",
    "                             max_length=150,\n",
    "                             length_penalty=20, \n",
    "                             num_beams=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"summary_ids\"]=result1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = []\n",
    "for i in range(len(train[\"summary_ids\"])):\n",
    "    result2.append(tokenizer.decode((train[\"summary_ids\"][i])[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = []\n",
    "for i in range(len(train[\"text\"])):\n",
    "    result3.append(len(tokenizer(train[\"text\"][i])['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"summary\"]=result2\n",
    "train[\"No.tokens\"]=result3\n",
    "train[[\"text\",\"No.tokens\",\"summary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.head(3)\n",
    "    '''\n",
    "    This function takes in a dataframe.\n",
    "    Then it returns a dataframe that creates a \"text\" for the summarized text.\n",
    "\n",
    "    Args:\n",
    "        source training data\n",
    "    \n",
    "    Returns:\n",
    "        data with a \"text\" containing the summarized text\n",
    "    '''\n",
    "    result = []\n",
    "    for i in range(len(data[\"text\"])):\n",
    "        result.append(tokenizer.encode(\"summarize: \"+data[\"text\"][i], return_tensors='pt', \n",
    "                                      max_length=tokenizer.model_max_length,\n",
    "                                      truncation=True))\n",
    "    \n",
    "    data[\"tokens_input\"] = result\n",
    "    \n",
    "    \n",
    "    result_1 = []\n",
    "    for i in range(len(data[\"tokens_input\"])):\n",
    "        result_1.append(model.generate(data[\"tokens_input\"][i], min_length=80, \n",
    "                                      max_length=150, length_penalty=15, \n",
    "                                     early_stopping=True))\n",
    "    \n",
    "    data[\"summary_ids\"] = result_1\n",
    "    \n",
    "    \n",
    "    result_2 = []\n",
    "    for i in range(len(data[\"summary_ids\"])):\n",
    "        result_2.append(tokenizer.decode((data[\"summary_ids\"][i])[0], skip_special_tokens=True))\n",
    "        \n",
    "    \n",
    "    data[\"summary\"]=result_2\n",
    "    \n",
    "    data = data.drop([\"tokens_input\", \"summary_ids\"], axis=1)\n",
    "    data = data[[\"text\",\"summary\"]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_text(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Q & A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = '''Indicators for targets 9.b and 9.c have data available. Globally, \n",
    "energy efficiency and the use of cleaner fuels and technologies reduced carbon dioxide emissions \n",
    "per unit of value added by 13 per cent between 2000 and 2013. Although expenditure on research and \n",
    "development continues to grow globally, the poorest countries, especially those in Africa, spend a very small \n",
    "proportion of their GDP on such expenditure. In 2013, global investment in research and development stood at $1.7 trillion \n",
    "(purchasing power parity), up from $732 billion in 2000.\n",
    "this is labeled as '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(passage=passage, question=\"how much was the investment?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"best_span_str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frames (Text Summrization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = data.head(3)\n",
    "    '''\n",
    "    This function takes in a dataframe.\n",
    "    Then it returns a dataframe that creates a \"text\" for the summarized text.\n",
    "\n",
    "    Args:\n",
    "        source training data\n",
    "    \n",
    "    Returns:\n",
    "        data with a \"text\" containing the summarized text\n",
    "    '''\n",
    "    result = []\n",
    "    for i in range(len(data[\"text\"])):\n",
    "        result.append(tokenizer.encode(\"summarize: \"+data[\"text\"][i], return_tensors='pt', \n",
    "                                      max_length=tokenizer.model_max_length,\n",
    "                                      truncation=True))\n",
    "    \n",
    "    data[\"tokens_input\"] = result\n",
    "    \n",
    "    \n",
    "    result_1 = []\n",
    "    for i in range(len(data[\"tokens_input\"])):\n",
    "        result_1.append(model.generate(data[\"tokens_input\"][i], min_length=80, \n",
    "                                      max_length=150, length_penalty=15, \n",
    "                                     early_stopping=True))\n",
    "    \n",
    "    data[\"summary_ids\"] = result_1\n",
    "    \n",
    "    \n",
    "    result_2 = []\n",
    "    for i in range(len(data[\"summary_ids\"])):\n",
    "        result_2.append(tokenizer.decode((data[\"summary_ids\"][i])[0], skip_special_tokens=True))\n",
    "        \n",
    "    \n",
    "    data[\"summary\"]=result_2\n",
    "    \n",
    "    data = data.drop([\"tokens_input\", \"summary_ids\"], axis=1)\n",
    "    data = data[[\"text\",\"summary\"]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_text(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qesAns(df,questions = \"Is Shell implicated?\"):\n",
    "\n",
    "        def qestions(df,column,question):\n",
    "            result = []\n",
    "            for i in range(len(train[column])):\n",
    "                result.append(predictor.predict(passage=train[column][i], question=question)[\"best_span_str\"])\n",
    "            return result\n",
    "\n",
    "        train[questions] = qestions(df,column= \"text\", question = questions)\n",
    "        return train[questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qesAns(train,questions= \"Is Shell implicated?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering different questions from predefined options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = ''\n",
    "info = \"\"\"Please choose an option below \\n\n",
    "a = what is the cost and monetary value\\n\n",
    "b = what technology was utilized?\\n\n",
    "c = what resources was utilized?\\n\n",
    "\"\"\"\n",
    "while option != 'a' and option != 'b' and option != 'c':\n",
    "    print(info)\n",
    "    option = input(info)\n",
    "\n",
    "print (\"Question \"+ \"\\\"\" +option+\"\\\"\" +\" chosen\")\n",
    "print (\"Loading-----------------------------------\")\n",
    "if option == \"a\":\n",
    "    option = \"what is the cost and monetary value\"\n",
    "elif option == \"b\":\n",
    "    option = \"what technology was used?\"\n",
    "elif option == \"c\":\n",
    "    option = \"what resources used?\"\n",
    "\n",
    "def qesAns(df,questions = option):\n",
    "    \"\"\"question and answer node meant to produce answers to artiles in a dataframe\n",
    "\n",
    "    Args:\n",
    "        data: Data containing a text column.\n",
    "    Returns:\n",
    "        data: a dataframe answering the asked question based on the articles in each row \n",
    "    \"\"\"\n",
    "    def qestions(df,column,question):\n",
    "        result = []\n",
    "        for i in range(len(train[column])):\n",
    "            result.append(predictor.predict(passage=train[column][i], question=question)[\"best_span_str\"])\n",
    "        return result\n",
    "\n",
    "    train[questions] = qestions(df,column= \"text\", question = questions)\n",
    "    return train[questions]\n",
    "qesAns(train,questions= option)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering different questions from predefined options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = 'b'\n",
    "info = \"\"\"Please choose an option below \\n\n",
    "a = what is the cost and monetary value\\n\n",
    "b = what technology was utilized?\\n\n",
    "c = what resources was utilized?\\n\n",
    "\"\"\"\n",
    "while option != 'a' and option != 'b' and option != 'c':\n",
    "    print(info)\n",
    "    # option = input(info)\n",
    "\n",
    "print (\"Question \"+ \"\\\"\" +option+\"\\\"\" +\" chosen\")\n",
    "print (\"Loading-----------------------------------\")\n",
    "if option == \"a\":\n",
    "    option = \"what is the cost and monetary value\"\n",
    "elif option == \"b\":\n",
    "    option = \"what technology was used?\"\n",
    "elif option == \"c\":\n",
    "    option = \"what resources used?\"\n",
    "\n",
    "def qesAns(df,questions = option):\n",
    "    \"\"\"question and answer node meant to produce answers to artiles in a dataframe\n",
    "\n",
    "    Args:\n",
    "        data: Data containing a text column.\n",
    "    Returns:\n",
    "        data: a dataframe answering the asked question based on the articles in each row \n",
    "    \"\"\"\n",
    "    def qestions(df,column,question):\n",
    "        result = []\n",
    "        for i in range(len(train[column])):\n",
    "            result.append(predictor.predict(passage=train[column][i], question=question)[\"best_span_str\"])\n",
    "        return result\n",
    "\n",
    "    train[questions] = qestions(df,column= \"text\", question = questions)\n",
    "    return train[questions]\n",
    "qesAns(train,questions= option)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_models = [\n",
    "    { 'name' : 'ner-model',\n",
    "      'url': 'https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz'\n",
    "    },\n",
    "    # { 'name' : 'ner-elmo',\n",
    "    #   'url' : 'https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz',\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load models\n",
    "print (\"Loading models...\")\n",
    "for nlp_model in nlp_models:\n",
    "    print (\"Loading model :\", nlp_model['name'])\n",
    "    nlp_model['model'] = Predictor.from_path(nlp_model['url'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locationOrganization(train):\n",
    "        train = train.head(3)\n",
    "        def entity_recognition (sentence):\n",
    "            location = []\n",
    "            for nlp_model in nlp_models:\n",
    "                results =  nlp_model['model'].predict(sentence=sentence)\n",
    "                for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "                    if tag != 'U-LOC':\n",
    "                        continue\n",
    "                    else:\n",
    "                        # print([word])#(f\"{word}\")\n",
    "                        location.append(word)\n",
    "                # print()\n",
    "                return location\n",
    "\n",
    "        def entity_recognition_pe(sentence):\n",
    "            organisation = []\n",
    "            for nlp_model in nlp_models:\n",
    "                results =  nlp_model['model'].predict(sentence=sentence)\n",
    "                for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "                    if tag != 'U-ORG':\n",
    "                        continue\n",
    "                    else:\n",
    "                        # print([word])#(f\"{word}\")\n",
    "                        organisation.append(word)\n",
    "                # print()\n",
    "                return organisation\n",
    "        result = []\n",
    "        for i in range(len(train[\"text\"])):\n",
    "            result.append(list(set(entity_recognition(train[\"text\"][i]))))\n",
    "        re1 = []\n",
    "        for i in range(len(train[\"text\"])):\n",
    "            re1.append(list(set(entity_recognition_pe(train[\"text\"][i]))))\n",
    "        train[\"location\"]=result\n",
    "        train[\"organisation\"]=re1\n",
    "        train['location'] = [', '.join(map(str, l)) for l in train['location']]\n",
    "        train['organisation'] = [', '.join(map(str, l)) for l in train['organisation']]\n",
    "        return train[[\"text\",\"location\",\"organisation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start of Latest Changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\lista\\AppData\\Local\\Temp\\tmpen2lq2di\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "nlp_model = Predictor.from_path('https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('guardian_publications.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [32:24<00:00, 72.01s/it] \n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_organization(df_org):\n",
    "  df_ent = []\n",
    "  for _, row in pd.DataFrame({\"beg\": df_org.loc[lambda x: x[\"tags\"] == \"B-ORG\"].index.values,\n",
    "                \"end\": df_org.loc[lambda x: x[\"tags\"] == \"L-ORG\"].index.values + 1}).iterrows():\n",
    "    df_ent.append(df_org.iloc[row[\"beg\"]:row[\"end\"]][\"words\"].str.cat(sep=\" \"))\n",
    "  df_ent.extend(df_org.loc[lambda x: x[\"tags\"] == \"U-ORG\"][\"words\"].to_list())\n",
    "  return df_ent\n",
    "\n",
    "\n",
    "df_edges = []\n",
    "for _, row in tqdm(list(train.iterrows())):\n",
    "  df_org = []\n",
    "  sents = SpacySentenceSplitter().split_sentences(row[\"text\"])\n",
    "  for i, s in list(enumerate(sents)):\n",
    "    res = nlp_model.predict(\n",
    "      sentence=s\n",
    "    )\n",
    "    df_org.append(pd.DataFrame({\"tags\": res[\"tags\"], \"words\": res[\"words\"], \"text\": row[\"text\"]})\\\n",
    "    .loc[lambda x: x[\"tags\"].str.contains(\"ORG\")])\n",
    "\n",
    "  df_org = pd.concat(df_org).reset_index(drop=True)\n",
    "  df_ent = get_organization(df_org)\n",
    "  df_edges.append(pd.DataFrame({\"text\": row[\"text\"], \"organization\": df_ent}))\n",
    "\n",
    "\n",
    "df_org = pd.concat(df_edges)\n",
    "df_org = pd.DataFrame(df_org.groupby(df_org[\"text\"])[\"organization\"].apply(lambda x: ', '.join(np.unique(x.values.ravel()))).reset_index())\n",
    "result = df_org[\"organization\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     AFP News Agency, Fed, Federal Reserve, House, ...\n",
       "1                               Green Connection, Shell\n",
       "2     BP, Barclays, Brent, British Gas, Centrica, Ro...\n",
       "3     IBEX, Mark Sweney Shell, Reuters, Reuters Shel...\n",
       "4     Business Today Free, EU, European Commission, ...\n",
       "5     Business Today Free, First Utility, Newsletter...\n",
       "6     BG Group, Business Today Free, Newsletters, Sa...\n",
       "7     @BusinessDesk, BP, Business Today, Cornwall, C...\n",
       "8     @BusinessDesk Shell, British Gas, Business Tod...\n",
       "9     BG Group, Business Today Free, Newsletters, Re...\n",
       "10    British Gas, Business Debtline, Business Today...\n",
       "11    Associated Press, Chevron, Exxon, ExxonMobil, ...\n",
       "12    BP, C of E, Christian Climate Action, Church o...\n",
       "Name: organization, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_org[\"organization\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [17:44<00:00, 39.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.tokenizers.sentence_splitter import SpacySentenceSplitter\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "from tqdm import tqdm\n",
    "def get_location(df_loc):\n",
    "  df_ent = []\n",
    "  for _, row in pd.DataFrame({\"beg\": df_loc.loc[lambda x: x[\"tags\"] == \"B-LOC\"].index.values,\n",
    "                \"end\": df_loc.loc[lambda x: x[\"tags\"] == \"L-LOC\"].index.values + 1}).iterrows():\n",
    "    df_ent.append(df_loc.iloc[row[\"beg\"]:row[\"end\"]][\"words\"].str.cat(sep=\" \"))\n",
    "  df_ent.extend(df_loc.loc[lambda x: x[\"tags\"] == \"U-LOC\"][\"words\"].to_list())\n",
    "  return df_ent\n",
    "\n",
    "\n",
    "df_edges = []\n",
    "for _, row in tqdm(list(train.iterrows())):\n",
    "  df_loc = []\n",
    "  sents = SpacySentenceSplitter().split_sentences(row[\"text\"])\n",
    "  for i, s in list(enumerate(sents)):\n",
    "    res = nlp_model.predict(\n",
    "      sentence=s\n",
    "    )\n",
    "    df_loc.append(pd.DataFrame({\"tags\": res[\"tags\"], \"words\": res[\"words\"], \"text\": row[\"text\"]})\\\n",
    "    .loc[lambda x: x[\"tags\"].str.contains(\"LOC\")])\n",
    "\n",
    "  df_loc = pd.concat(df_loc).reset_index(drop=True)\n",
    "  df_ent = get_location(df_loc)\n",
    "  df_edges.append(pd.DataFrame({\"text\": row[\"text\"], \"location\": df_ent}))\n",
    "\n",
    "\n",
    "df_loc = pd.concat(df_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shell’s long-serving chief executive, Ben van ...</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shell’s long-serving chief executive, Ben van ...</td>\n",
       "      <td>Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shell’s long-serving chief executive, Ben van ...</td>\n",
       "      <td>Netherlands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shell’s long-serving chief executive, Ben van ...</td>\n",
       "      <td>London</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shell’s long-serving chief executive, Ben van ...</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shell made record profits of nearly £10bn betw...</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shell made record profits of nearly £10bn betw...</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shell made record profits of nearly £10bn betw...</td>\n",
       "      <td>Kremlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Shell made record profits of nearly £10bn betw...</td>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shell made record profits of nearly £10bn betw...</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     location\n",
       "0   Shell’s long-serving chief executive, Ben van ...       London\n",
       "1   Shell’s long-serving chief executive, Ben van ...       Europe\n",
       "2   Shell’s long-serving chief executive, Ben van ...  Netherlands\n",
       "3   Shell’s long-serving chief executive, Ben van ...       London\n",
       "4   Shell’s long-serving chief executive, Ben van ...           UK\n",
       "..                                                ...          ...\n",
       "4   Shell made record profits of nearly £10bn betw...       Russia\n",
       "5   Shell made record profits of nearly £10bn betw...       Russia\n",
       "6   Shell made record profits of nearly £10bn betw...      Kremlin\n",
       "7   Shell made record profits of nearly £10bn betw...      Ukraine\n",
       "8   Shell made record profits of nearly £10bn betw...       Russia\n",
       "\n",
       "[205 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc2 = df_loc.copy()\n",
    "df_loc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Algoa</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All Saints</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ascot</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baltic Sea</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Barisal</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beirut</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Britain</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Canada</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Europe</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Germany</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great Britain</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Guyana</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Indian Ocean</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Irving</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Korea</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kremlin</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>London</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Makhanda</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>North Sea</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Norway</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Paris</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Permian Basin</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Russia</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Slovenia</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Southwark</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Spain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Texas</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Transkei</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>UK</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>US</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>United Reformed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Wild Coast</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           location  counts\n",
       "0             Algoa       2\n",
       "1        All Saints       2\n",
       "2             Ascot       2\n",
       "3        Baltic Sea       2\n",
       "4        Bangladesh       4\n",
       "5           Barisal       2\n",
       "6            Beirut       2\n",
       "7           Belgium       2\n",
       "8           Britain       8\n",
       "9            Canada       2\n",
       "10           Europe      17\n",
       "11          Germany       4\n",
       "12    Great Britain       2\n",
       "13           Guyana       2\n",
       "14     Indian Ocean       2\n",
       "15           Irving       2\n",
       "16            Korea       2\n",
       "17          Kremlin       2\n",
       "18           London       5\n",
       "19         Makhanda       2\n",
       "20      Netherlands       3\n",
       "21       New Mexico       2\n",
       "22        North Sea       4\n",
       "23           Norway       6\n",
       "24            Paris       2\n",
       "25    Permian Basin       2\n",
       "26           Russia      28\n",
       "27         Slovenia       2\n",
       "28     South Africa       6\n",
       "29        Southwark       2\n",
       "30            Spain       1\n",
       "31            Texas       4\n",
       "32         Transkei       2\n",
       "33               UK      24\n",
       "34               US      27\n",
       "35          Ukraine      18\n",
       "36  United Reformed       2\n",
       "37       Wild Coast       2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_loc2 = pd.DataFrame(df_loc2.groupby(df_loc2[\"text\"])[\"location\"].apply(lambda x: ', '.join(np.unique(x.values.ravel()))).reset_index(drop=True, inplace=True))\n",
    "df_loc.reset_index(drop=True, inplace=True)\n",
    "#df_loc2[\"location\"]\n",
    "df2 = df_loc2.groupby(['location'])['location'].size().reset_index(name='counts')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def locationOrganization(data):\n",
    "    data= data.head(5)\n",
    "    def entity_recognition (sentence):\n",
    "        location = []\n",
    "        organisation = []\n",
    "        results =  nlp_model.predict(sentence=sentence)\n",
    "        for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "            if tag == 'U-ORG':\n",
    "                organisation.append(word)               \n",
    "            elif tag == 'U-LOC':\n",
    "                location.append(word)\n",
    "            else: \n",
    "                continue\n",
    "                \n",
    "            # print()\n",
    "            data2=pd.DataFrame({\"location\":location, \"organisation\":organisation})\n",
    "            return location, organisation       \n",
    "                \n",
    "    result = pd.DataFrame()\n",
    "    for i in range(len(train[\"text\"])):\n",
    "        result.append(entity_recognition(train[\"text\"][i]))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = locationOrganization(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End of Latest Changes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "a_list.extend(train['location'].tolist())\n",
    "# a_list.extend(train['country'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ()\n",
    "for values in train.location.iteritems():\n",
    "        x += values\n",
    "\n",
    "passage=str(x)\n",
    "passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage1 = passage.replace(\"'\", \"\")\n",
    "# passage1 = passage.replace(\"(\", \"\")\n",
    "# passage1 = passage.replace(\")\", \"\")\n",
    "passage1 = ''.join([i for i in passage1 if not i.isdigit()])\n",
    "passage1 = passage1\n",
    "passage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_recognition (sentence):\n",
    "    miscellaneous = []\n",
    "    person = []\n",
    "    organisation = []\n",
    "    loc = []\n",
    "    for nlp_model in nlp_models:\n",
    "        results =  nlp_model['model'].predict(sentence=sentence)\n",
    "        for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "            if tag != 'U-LOC':\n",
    "                continue\n",
    "            else:\n",
    "                loc.append(word)\n",
    "        print(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_recognition (passage1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installation\n",
    "# pip install pycountry-convert\n",
    "#function to convert to alpah2 country codes and continents\n",
    "from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2\n",
    "def get_continent(col):\n",
    "    try:\n",
    "        cn_a2_code =  country_name_to_country_alpha2(col)\n",
    "    except:\n",
    "        cn_a2_code = 'Unknown' \n",
    "    try:\n",
    "        cn_continent = country_alpha2_to_continent_code(cn_a2_code)\n",
    "    except:\n",
    "        cn_continent = 'Unknown' \n",
    "    return (cn_a2_code, cn_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installation\n",
    "# pip install geopy\n",
    "#function to get longitude and latitude data from country name\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "def geolocate(country):\n",
    "    try:\n",
    "        # Geolocate the center of the country\n",
    "        loc = geolocator.geocode(country)\n",
    "        # And return latitude and longitude\n",
    "        return (loc.latitude, loc.longitude)\n",
    "    except:\n",
    "        # Return missing value\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installation\n",
    "# pip install folium\n",
    "# Create a world map to show distributions of users \n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "#empty map\n",
    "world_map= folium.Map(tiles=\"cartodbpositron\")\n",
    "marker_cluster = MarkerCluster().add_to(world_map)\n",
    "#for each coordinate, create circlemarker of user percent\n",
    "for i in range(len(df)):\n",
    "        lat = df.iloc[i]['Latitude']\n",
    "        long = df.iloc[i]['Longitude']\n",
    "        radius=5\n",
    "        popup_text = \"\"\"Country : {}<br>\n",
    "                    %of Users : {}<br>\"\"\"\n",
    "        popup_text = popup_text.format(df.iloc[i]['Country'],\n",
    "                                   df.iloc[i]['User_Percent']\n",
    "                                   )\n",
    "        folium.CircleMarker(location = [lat, long], radius=radius, popup= popup_text, fill =True).add_to(marker_cluster)\n",
    "#show the map\n",
    "world_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entity_recognition (sentence):\n",
    "#     miscellaneous = []\n",
    "#     person = []\n",
    "#     organisation = []\n",
    "#     location = []\n",
    "#     for nlp_model in nlp_models:\n",
    "#         results =  nlp_model['model'].predict(sentence=sentence)\n",
    "#         for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "#             if tag != 'U-LOC':\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 # print([word])#(f\"{word}\")\n",
    "#                 location.append(word)\n",
    "#         # print()\n",
    "#         return location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entity_recognition_per (sentence):\n",
    "#     miscellaneous = []\n",
    "#     person = []\n",
    "#     organisation = []\n",
    "#     location = []\n",
    "#     for nlp_model in nlp_models:\n",
    "#         results =  nlp_model['model'].predict(sentence=sentence)\n",
    "#         for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "#             if tag != 'U-ORG':\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 # print([word])#(f\"{word}\")\n",
    "#                 organisation.append(word)\n",
    "#         # print()\n",
    "#         return organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16439ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = '''Ben van Beurden, who took over in 2014, would leave Shell in the middle of the most severe energy crisis of his tenure. his departure would end a near-40-year career at the oil and gas giant in England and Ghana.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d81b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_recognition_per(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a984516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# for i in range(len(train[\"text\"])):\n",
    "#     result.append(entity_recognition_per(train[\"text\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a984516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"org\"]=result\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fb92d5742c001c91c37d9cc578d20237819f1079598661672d9831a3f737940"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
